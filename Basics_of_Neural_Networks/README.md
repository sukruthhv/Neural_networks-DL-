# Basics of Neural Networks
In this repo, i am going to share the knowledge and understanding about the basics of Artificial Neural Networks(ANN), these are some of the concepts that would be touch based upon :

 - Neural Networks basics (Nodes, Hidden Layers, Input and Output layers,Epochs)
 - Gradient Descent
 - Activation functions
 - Dropout layers
 - Regularization
 - Batch Normalization
 - Callback functions

## Neural Network Basics
![enter image description here](https://www.smartsheet.com/sites/default/files/IC-simplified-artificial-neural-networks-corrected.svg)
Let me try to explain the basic terminology of the Neural Networks through the picture above as reference.

 - **Nodes :** These are the basic computational entities in the Neural Network. This is the point, where the computation (DOT products takes place). Now in the above picture, each of the circles in the Hidden Layer are Nodes.
 - **Input Layer :** This is the layer, where each node represents the features of the Dataset, For instance if you take the Titanic Dataset, then the Name is one feature, Boarding point, Age, Sex and each of these features are represented as nodes of the I/P layer.
 - **Hidden Layers:** I would say that Hidden layers are the computational powerhouses of the NN, where the dot products of weights and features are taken place (infact Biases are added for each neuron). We can add 'n' no. of hidden layers in a given NN architecture. Apart from that, majorly the HL are responsible for introducing the ***non-linearity in the flow***.
 - **Forward Propagation:** Is the flow, where each node/neuron in the HL would compute the DOT product , add the bias and find the Loss (y-yhat).
 - **Backward propagation:** Is the flow, wherein the Loss function would calculate the new weights using ***Gradient Descent*** based on the Loss incurred during the Forward propagation.
 - Epochs : The combination of 1 Forward and Backward propagation is called teh epochs, simply put, they are iterations.

***

> Simply stated, the Neural networks are the complex form of Linear
> Regression with Non-linearity introduced on the Linear features.

***
**Gradient Descent:** This is very important concept to understand for any regression analysis w.r.t Loss computation. 
![enter image description here](https://i.ytimg.com/vi/aR0M-xtcaaU/hqdefault.jpg)

Consider the above graph, where the X-axis represent the random weights that gets initialized to compute the DOT products at the HL (w1*x1+w2*x2+w3*x3+....), all the w's are called weights. Now these weights are initialized randomly. The Y-axis is basically the Loss function denoted as 'J'. The aim here is to attain the Minimal Loss "J" for the given weights. 
How do we do this? We basically do a differentiation of Loss w.r.t weights with some learning rate "alpha". Basically we are finding the slope here and trying to achieve the minima.

**Activation functions:**
As mentioned above, we have to introduce the non-linearity in the architecture and that is achieved through activation functions. To name few, "Tanh", "Relu".
Relu : This activation function woudl basically do the below transformation of linearity :
![enter image description here](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQUAAADBCAMAAADxRlW1AAAAsVBMVEX////Ly8u1tbUAbbGqw91Uj8F1dXUAaq8AcLIAbrEPc7MAAADOzs7i4uIAaa/v7++6urrY2NixsbH19fWbm5t7e3uTk5Py9vrN2+p0ocrd5/GXt9a4zeLk7PTFxcVSUlJfX18zMzNGRkaoqKiAqM5nZ2c4gbqLi4vD1OZvb2+StNQ0f7lpm8dJib4heLZbk8MqKiocHBw6OzpJSUkQDw/H2OihvtkKZ6E0d6xiiq9+enYp88KPAAAHB0lEQVR4nO2daXuiOgBGI7ayCIICLnXqRq21+7Sdzr33//+wK5u2DZGEJRucD2NL85BwBjC8iRGAlhbqeL8tZRH9tLMZN4UhbzZ4B4fjdwFYsW4LM8yP3SIAvmf/BmDpRpssl3GbGLABOxOA98MpAcbxleE5jJvEgA3wh4d7wis4Hv7SZ9siFryFB2+uFj0Q6TjQ/2DcJAaMgdUDvhu+Q6yjDYt+32LcJiYs4xfTi178ft9j2Bhm2J3oZRT/9r5e7xk2hhcck3ULeEDpsG4BD7QWQloLIa2FkNZCSGshpLUQ0loIabQFSxnFkVujLexNMw6ZHBYWfjGoM4vNLnqcWig+g+eI2WBGv9IsAjAOs6ahuaNv4UJVOTkZFmnAQP+KuOpqN7TrRLAaJxk0/bvjs3FLu0okaQBP3cKLoU8oV5kPbQvzrvqHbo04ULYwVfV7qhXiQdfCRDMuadaHC10LT4bB300BULZwo6lXFKvDh6aFX2r3gl5tJFC0cK1rj9QqI4OihQfjgVpdhNCz8Kh1r2nVRQo1C59d9Y5SVeTQsnClcvMMlQElCxPDeKJSUTEoWbjktLuUQMfCvaZOadRTFCoW/qj6J4VqikPDwkTXXuqvpQw0LNwaz/VXUgoKFmaaym13KaF+CxddXiJnNLVbOHSXOBl9OEPtFniKnJHUbeHF0HjuLiXUbGGu8xg5n0iOvl4LnEbOR7xkJnCtFniNnFOG3jZ8sSyvztHaJ8Ooce/l2cefGHKCfY0WbnROI+eEzmr/EV8KNY5Z8xs5n6j9vnAtQncpoT4LHEfOELVZeNR03p+hTtRl4VPnOHKGqMkC35EzRD0WOI+cIeqxwHnkDFGLBd4jZ4g6LNzxHjlD1GBhwu8IPYoaLPAfOUNUb2HG8Qg9isotiBA5Q1RtQbDuUkLVFoSInCEqtsDlLOd8qrUw1wdcR84oKrUwHfAdOSOp0gLvkTOaKi08idddSqjQAreznPOpzoIIkTOKyixcd8WJnCEqs/AsUOQMUZUFoSLnI+laVRVZECtyTlkFlY5NifkMBYagF54NrhtUYUG0yPnI+C38N9itqhizFi1yTrHBIoh+qGLM+l4biBU5p6ytbbyyYQX3BfEi5xR3lLxJlLcw0YSLnCHKWxAwcoYobUGAWc75lLXwi5uFNcpQ0oKg3aWflLQgZuQMUc6CoJEzRCkLvM9yxqaMBd5nOeNTwoK4kTNECQviRs4QxS0IHDlDFLYgcuQMUdTCdVf8Z6gTRS2INMs5n4IWxIyckRSzIGbkjKaQhakcz1AnilgQNnJGUsTCpRCfFCWhgAXhZjnnQ25B3MgZDbEF/hfWKACxBRki5yPpl/GRWhBxljOS7XhZaIV0IWc5I3GAE3/XEpkFSSLnE9GY9Xi/IRqzliRyTrE3yYLQRGPWYiysgU9/0SG/L0gTOSe4juPEg9YEFuSJnCHwLUw0Q77uUgK+Bd4X1igDtgWZImcIXAtSRc4QmBZEWlijAJgW5IqcIfAsSBY5Q2BZ+NQHUkXOEDgWpgNN1u5SAoYF+SJnCAwLos5yJiDfgoSRM0SuBRkjZ4g8CwIurFGAPAtSRc5IcixIMcs5n/MW5Iqc0Zy1IF3kjOKsBckiZzTnLMgWOaM5Y2GuC/qhMHLQFqaqPqfZEpYgLcgcOUMgLUg0yxmNo8SvKAs32kDeyDklWCYWEOOUckfOR4bRwP14/5Y5Zi30whoEmPH0BcS50JTukhl/2Dz7vsDz1wdWipVcCVkWZJvlnE+Ghakqe+QMAVtoQOQMAVtoQOQMAVloQuQM8dNCIyJniB8W5PpQGDY/LDQjcob4bqEhkTPENwtNiZwhvlpoTOQM8dXCc/O6SwlfLDQncoY4WZjrDewuJRwtNClyhkgtTPQGRc4QqYVGRM5IEgtSz3LOJ7bQkMgZSWRB8lnO+UQWHhoSOSMJLcg+yzmfg4XmRc4QinnVvMgZQlk8S7MOWXEWfzmLnO8uGDAzGDxDWQv032aDbiaanr09G10jKX3YOYPI2ez3l4GV/bf7p8tM/snejIKs+O2///Xw8V99gtK97Sa7+KofsnNJzI2JSoMeUWmFbJ34PVFpO8jebvY3HuJUQELmjLChI7JVKLZEpS0ve7tLdkQtLS1ZjBAXWCbWanvm/fg7fm9MtOe9Q1Ac2G+oah2wJr432O8kFg435jVmUWsHfDu/2HHPLvaeI/ZL1F/WW5K7csc0zSHYo262P7APxcOVRhRcaYsR8IjeKp0dQWHPRloIPkhqHSnKqOP54xXW+WOFxW0Q+Li7Dy0MSZpD0hmx1s4rYufu2iM5uyNsS8E+rgMj38U9y90VQP53ZeDssfcc7tyyUBf/ygIb0m7TwQPJaTvyxtiiTZ/kVFAI9hw3hah0S0tLS0tLJv8DZexcx3VKqKcAAAAASUVORK5CYII=)
"Tanh": Similarly Tanh would introduce the non-linearity as mentioned in the graph below :
![enter image description here](data:image/png;base64,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)
It tries to get the 'y' between -1 to +1.

**Dropout layers:**
Lets consider a scenario where in certain neurons/nodes are given heavy weightage and due to which during the training phase, only those neurons get to be trained and other neurons would be more like a dummy, which doesn't get a chance to learn and contribute. Hence we introduce a layer called Dropout with certain probability, which drops the neurons randomly using the probability or dropout ratio, due to which the neurons which didn't had a chance before to learn would now have better chances to learn, due to which all the neurons would learn and contribute in the cluster.
![enter image description here](http://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif)
As we can see above that, for a given dropout ratio of 0.5, out of 4 nodes in an HL, only 2 nodes gets activated everytime (in each epoch???) and hence all the neurons get fair chance to learn and contribute. (***Note: During the dropout those neurons being dropped would have the input and o/p connection being cut off***).

**Regularization:**
Think about a scenario, where in Decision Trees, we found that the overfitting was a general scenario as we go the finest boundaries, where the entropy was literally 0. This is the best example for Overfit issue. This is one of the issues tackled by the RF by pruning the trees, so that we are not over fitting the model and make it more generalized. That is the same issue we can face in Deep Neural networks as well, hence we need to regularize the models, so that we can avoid overfitting. 
How do we do this?
![enter image description here](https://jamesmccaffrey.files.wordpress.com/2017/06/l2_regularization_equations.jpg)This can be achieved by adding the Loss function/MSE error function with weights. Now how does this help? Think as when does the overfitting happen in NN? When the GD optimizer tries to get the Loss to absolutely 0 (or very close to 0 - analogy is to think about DT, which tries to overfit with absolute boundaries!!), now if we add a +ve weights, the Loss Function of error would always have some residue and hence the GD wouldn't achieve the '0' error ever and hence the model could be more generalized now as we have avoided overfitting.
Above scenario/image demonstrates the L2 regularization, where the weights square has been taken, with L1, we would be just taking absolute values of weights (non-squared)

> Adding regularization would eliminate the need for Dropout layers as with regularization, all the neurons would be given more weightage!!!

**Batch Normalization:**
If we remember as why we used to do Standard scaling across features in Regression problems, then with Batch normalization, we achieve very similar intent. Since we are passing the features (x1,x2,x3..xn) to the hidden layers, if we do not normalize them, then the weights can be scaled differently for different features and hence causing the issue while validation/testing. Like consider that we have features like weights in KGS and height in cms, then the scaling is improper and we may end up with equations like this yhat=x1(in KGS)*100000+x2(in cms)*10. In this case, for every single unit raise in x1, we see a huge jump in yhat, whereas with every single unit raise in x2, it would be small raise in yhat, which is not fair, hence we scale it to bring them to same scale and hence the weights would be more or less in a very similar scale.
This is achieved again by introducing batch normalization in the Neural network layers.
![enter image description here](https://miro.medium.com/max/1596/1*rXY5zJrDdHv6EdKhJvKqcA.png)
So, we understand that this is called Normalization, what does the name "Batch" suggest here? Right!!, so we basically split the rows/dataset elements in Mini batches, which would go through the forward propagation, predict, generate the Loss, which is fed into GD optimizer, get the corrected weights as part of the Backward propagation and take up the next batch and apply these new weights to the next batch. In this way the subsequent batches would ***converge to minimal error much faster than taking the whole batch at once***. This is as well one of the major advantage in Batch normalization.

**Callback functions:**
Callback functions are very useful to achieve a functionality after each batch/epoch. For instance, consider that we are achieving the Validation loss converging to Training loss at much earlier epochs, then why waste the resource by running all the defined epochs? This is where callback functions like EarlyStopping can be very beneficial, where we are calling/invoking the call back function as soon as we see the convergence. More about this can be seen in the code.

**Co-Variance shift:**
Whenever we train the models, we should make sure that we are normalizing them, else we may see an adverse results, when we use them in production, provided the test data in Prod is not normalized. For instance consider that you have trained for black cats, then your model shouldn't go wrong in Production, when give white cats, hence we normalize them to shift to the same scale (x-u/sigma). 
