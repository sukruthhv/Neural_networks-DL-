# Basics of Neural Networks
In this repo, i am going to share the knowledge and understanding about the basics of Artificial Neural Networks(ANN), these are some of the concepts that would be touch based upon :

 - Neural Networks basics (Nodes, Hidden Layers, Input and Output layers,Epochs)
 - Gradient Descent
 - Activation functions
 - Dropout layers
 - Regularization
 - Batch Normalization
 - Callback functions

## Neural Network Basics
![enter image description here](https://www.smartsheet.com/sites/default/files/IC-simplified-artificial-neural-networks-corrected.svg)
Let me try to explain the basic terminology of the Neural Networks through the picture above as reference.

 - **Nodes :** These are the basic computational entities in the Neural Network. This is the point, where the computation (DOT products takes place). Now in the above picture, each of the circles in the Hidden Layer are Nodes.
 - **Input Layer :** This is the layer, where each node represents the features of the Dataset, For instance if you take the Titanic Dataset, then the Name is one feature, Boarding point, Age, Sex and each of these features are represented as nodes of the I/P layer.
 - **Hidden Layers:** I would say that Hidden layers are the computational powerhouses of the NN, where the dot products of weights and features are taken place (infact Biases are added for each neuron). We can add 'n' no. of hidden layers in a given NN architecture. Apart from that, majorly the HL are responsible for introducing the ***non-linearity in the flow***.
 - **Forward Propagation:** Is the flow, where each node/neuron in the HL would compute the DOT product , add the bias and find the Loss (y-yhat).
 - **Backward propagation:** Is the flow, wherein the Loss function would calculate the new weights using ***Gradient Descent*** based on the Loss incurred during the Forward propagation.
 - Epochs : The combination of 1 Forward and Backward propagation is called teh epochs, simply put, they are iterations.

***

> Simply stated, the Neural networks are the complex form of Linear
> Regression with Non-linearity introduced on the Linear features.

***
**Gradient Descent:** This is very important concept to understand for any regression analysis w.r.t Loss computation. 
![enter image description here](https://i.ytimg.com/vi/aR0M-xtcaaU/hqdefault.jpg)
Consider the above graph, where the X-axis represent the random weights that gets initialized to compute the DOT products at the HL (w1*x1+w2*x2+w3*x3+....), all the w's are called weights. Now these weights are initialized randomly. The Y-axis is basically the Loss function denoted as 'J'. The aim here is to attain the Minimal Loss "J" for the given weights. 
How do we do this? We basically do a differentiation of Loss w.r.t weights with some learning rate "alpha". Basically we are finding the slope here and trying to achieve the minima.

**Activation functions:**
As mentioned above, we have to introduce the non-linearity in the architecture and that is achieved through activation functions. To name few, "Tanh", "Relu".
Relu : This activation function woudl basically do the below transformation of linearity :
![enter image description here](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQUAAADBCAMAAADxRlW1AAAAsVBMVEX////Ly8u1tbUAbbGqw91Uj8F1dXUAaq8AcLIAbrEPc7MAAADOzs7i4uIAaa/v7++6urrY2NixsbH19fWbm5t7e3uTk5Py9vrN2+p0ocrd5/GXt9a4zeLk7PTFxcVSUlJfX18zMzNGRkaoqKiAqM5nZ2c4gbqLi4vD1OZvb2+StNQ0f7lpm8dJib4heLZbk8MqKiocHBw6OzpJSUkQDw/H2OihvtkKZ6E0d6xiiq9+enYp88KPAAAHB0lEQVR4nO2daXuiOgBGI7ayCIICLnXqRq21+7Sdzr33//+wK5u2DZGEJRucD2NL85BwBjC8iRGAlhbqeL8tZRH9tLMZN4UhbzZ4B4fjdwFYsW4LM8yP3SIAvmf/BmDpRpssl3GbGLABOxOA98MpAcbxleE5jJvEgA3wh4d7wis4Hv7SZ9siFryFB2+uFj0Q6TjQ/2DcJAaMgdUDvhu+Q6yjDYt+32LcJiYs4xfTi178ft9j2Bhm2J3oZRT/9r5e7xk2hhcck3ULeEDpsG4BD7QWQloLIa2FkNZCSGshpLUQ0loIabQFSxnFkVujLexNMw6ZHBYWfjGoM4vNLnqcWig+g+eI2WBGv9IsAjAOs6ahuaNv4UJVOTkZFmnAQP+KuOpqN7TrRLAaJxk0/bvjs3FLu0okaQBP3cKLoU8oV5kPbQvzrvqHbo04ULYwVfV7qhXiQdfCRDMuadaHC10LT4bB300BULZwo6lXFKvDh6aFX2r3gl5tJFC0cK1rj9QqI4OihQfjgVpdhNCz8Kh1r2nVRQo1C59d9Y5SVeTQsnClcvMMlQElCxPDeKJSUTEoWbjktLuUQMfCvaZOadRTFCoW/qj6J4VqikPDwkTXXuqvpQw0LNwaz/VXUgoKFmaaym13KaF+CxddXiJnNLVbOHSXOBl9OEPtFniKnJHUbeHF0HjuLiXUbGGu8xg5n0iOvl4LnEbOR7xkJnCtFniNnFOG3jZ8sSyvztHaJ8Ooce/l2cefGHKCfY0WbnROI+eEzmr/EV8KNY5Z8xs5n6j9vnAtQncpoT4LHEfOELVZeNR03p+hTtRl4VPnOHKGqMkC35EzRD0WOI+cIeqxwHnkDFGLBd4jZ4g6LNzxHjlD1GBhwu8IPYoaLPAfOUNUb2HG8Qg9isotiBA5Q1RtQbDuUkLVFoSInCEqtsDlLOd8qrUw1wdcR84oKrUwHfAdOSOp0gLvkTOaKi08idddSqjQAreznPOpzoIIkTOKyixcd8WJnCEqs/AsUOQMUZUFoSLnI+laVRVZECtyTlkFlY5NifkMBYagF54NrhtUYUG0yPnI+C38N9itqhizFi1yTrHBIoh+qGLM+l4biBU5p6ytbbyyYQX3BfEi5xR3lLxJlLcw0YSLnCHKWxAwcoYobUGAWc75lLXwi5uFNcpQ0oKg3aWflLQgZuQMUc6CoJEzRCkLvM9yxqaMBd5nOeNTwoK4kTNECQviRs4QxS0IHDlDFLYgcuQMUdTCdVf8Z6gTRS2INMs5n4IWxIyckRSzIGbkjKaQhakcz1AnilgQNnJGUsTCpRCfFCWhgAXhZjnnQ25B3MgZDbEF/hfWKACxBRki5yPpl/GRWhBxljOS7XhZaIV0IWc5I3GAE3/XEpkFSSLnE9GY9Xi/IRqzliRyTrE3yYLQRGPWYiysgU9/0SG/L0gTOSe4juPEg9YEFuSJnCHwLUw0Q77uUgK+Bd4X1igDtgWZImcIXAtSRc4QmBZEWlijAJgW5IqcIfAsSBY5Q2BZ+NQHUkXOEDgWpgNN1u5SAoYF+SJnCAwLos5yJiDfgoSRM0SuBRkjZ4g8CwIurFGAPAtSRc5IcixIMcs5n/MW5Iqc0Zy1IF3kjOKsBckiZzTnLMgWOaM5Y2GuC/qhMHLQFqaqPqfZEpYgLcgcOUMgLUg0yxmNo8SvKAs32kDeyDklWCYWEOOUckfOR4bRwP14/5Y5Zi30whoEmPH0BcS50JTukhl/2Dz7vsDz1wdWipVcCVkWZJvlnE+Ghakqe+QMAVtoQOQMAVtoQOQMAVloQuQM8dNCIyJniB8W5PpQGDY/LDQjcob4bqEhkTPENwtNiZwhvlpoTOQM8dXCc/O6SwlfLDQncoY4WZjrDewuJRwtNClyhkgtTPQGRc4QqYVGRM5IEgtSz3LOJ7bQkMgZSWRB8lnO+UQWHhoSOSMJLcg+yzmfg4XmRc4QinnVvMgZQlk8S7MOWXEWfzmLnO8uGDAzGDxDWQv032aDbiaanr09G10jKX3YOYPI2ez3l4GV/bf7p8tM/snejIKs+O2///Xw8V99gtK97Sa7+KofsnNJzI2JSoMeUWmFbJ34PVFpO8jebvY3HuJUQELmjLChI7JVKLZEpS0ve7tLdkQtLS1ZjBAXWCbWanvm/fg7fm9MtOe9Q1Ac2G+oah2wJr432O8kFg435jVmUWsHfDu/2HHPLvaeI/ZL1F/WW5K7csc0zSHYo262P7APxcOVRhRcaYsR8IjeKp0dQWHPRloIPkhqHSnKqOP54xXW+WOFxW0Q+Li7Dy0MSZpD0hmx1s4rYufu2iM5uyNsS8E+rgMj38U9y90VQP53ZeDssfcc7tyyUBf/ygIb0m7TwQPJaTvyxtiiTZ/kVFAI9hw3hah0S0tLS0tLJv8DZexcx3VKqKcAAAAASUVORK5CYII=)
"Tanh": Similarly Tanh would introduce the non-linearity as mentioned in the graph below :
![enter image description here](data:image/png;base64,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)
It tries to get the 'y' between -1 to +1.
